Algorithm 1 Iterative Group Relative Policy Optimization
Input initial policy model ğœ‹ğœƒinit ; reward models ğ‘Ÿğœ‘; task prompts D; hyperparameters ğœ€, ğ›½, ğœ‡
1: policy model ğœ‹ğœƒ â† ğœ‹ğœƒinit
2: for iteration = 1, . . . , I do
3: reference model ğœ‹ğ‘Ÿğ‘’ ğ‘“ â† ğœ‹ğœƒ
4: for step = 1, . . . , M do
5: Sample a batch Dğ‘ from D
6: Update the old policy model ğœ‹ğœƒğ‘œğ‘™ğ‘‘ â† ğœ‹ğœƒ
7: Sample ğº outputs {ğ‘œğ‘–}ğºğ‘–=1 âˆ¼ ğœ‹ğœƒğ‘œğ‘™ğ‘‘ (Â· | ğ‘) for each question ğ‘ âˆˆ Dğ‘
8: Compute rewards {ğ‘Ÿğ‘–}ğºğ‘–=1 for each sampled output ğ‘œğ‘– by running ğ‘Ÿğœ‘
9: Compute Ë†ğ´ğ‘–,ğ‘¡ for the ğ‘¡-th token of ğ‘œğ‘– through group relative advantage estimation.
10: for GRPO iteration = 1, . . . , ğœ‡ do
11: Update the policy model ğœ‹ğœƒ by maximizing the GRPO objective (Equation 21)
12: Update ğ‘Ÿğœ‘ through continuous training using a replay mechanism.
Output ğœ‹ğœƒ





Starting GRPO training following Deepseek GRPO Algorithm...

=== Iteration 1/10 ===
  Step 1/10: Loss=-0.0000, Success=100.00%, Î¼_iters=4
  Step 10/10: Loss=0.0000, Success=80.00%, Î¼_iters=4
Iteration 1 Summary:
  Average Loss: -0.0001
  Average Success Rate: 70.00%
  Average Reward: -205.83

=== Iteration 2/10 ===
  Step 1/10: Loss=-0.0000, Success=60.00%, Î¼_iters=4
  Step 10/10: Loss=0.0094, Success=80.00%, Î¼_iters=4
Iteration 2 Summary:
  Average Loss: -0.0000
  Average Success Rate: 56.00%
  Average Reward: -366.53

=== Iteration 3/10 ===
  Step 1/10: Loss=0.0114, Success=80.00%, Î¼_iters=4
  Step 10/10: Loss=0.0000, Success=60.00%, Î¼_iters=4
Iteration 3 Summary:
  Average Loss: 0.0014
  Average Success Rate: 72.00%
  Average Reward: -182.90

=== Iteration 8/10 ===
  Step 1/10: Loss=-0.0015, Success=60.00%, Î¼_iters=4
  Step 10/10: Loss=0.0000, Success=40.00%, Î¼_iters=4
Iteration 8 Summary:
  Average Loss: 0.0049
  Average Success Rate: 54.00%
  Average Reward: -390.43

=== Iteration 9/10 ===
  Step 1/10: Loss=0.0076, Success=60.00%, Î¼_iters=4
  Step 10/10: Loss=0.0000, Success=80.00%, Î¼_iters=4
Iteration 9 Summary:
  Average Loss: 0.0046
  Average Success Rate: 63.50%
  Average Reward: -271.55

=== Iteration 10/10 ===
  Step 1/10: Loss=-0.0002, Success=60.00%, Î¼_iters=4
  Step 10/10: Loss=-0.0002, Success=100.00%, Î¼_iters=4
Iteration 10 Summary:
  Average Loss: 0.0019
  Average Success Rate: 69.00%
  Average Reward: -214.97